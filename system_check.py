#!/usr/bin/env python3
"""
System Performance and Capability Report
ØªÙ‚Ø±ÙŠØ± Ø£Ø¯Ø§Ø¡ ÙˆÙ‚Ø¯Ø±Ø§Øª Ø§Ù„Ù†Ø¸Ø§Ù…

Analyzes system capabilities for AI/ML workloads without GPU
"""

import os
import sys
import psutil
import platform
import subprocess
from pathlib import Path

def get_system_info():
    """Get comprehensive system information"""
    print("ğŸ’» Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ù†Ø¸Ø§Ù…")
    print("System Information")
    print("=" * 30)
    
    info = {
        'platform': platform.system(),
        'platform_version': platform.version(),
        'architecture': platform.architecture()[0],
        'processor': platform.processor(),
        'python_version': platform.python_version(),
    }
    
    # CPU information
    info['cpu_count'] = psutil.cpu_count(logical=False)
    info['cpu_count_logical'] = psutil.cpu_count(logical=True)
    info['cpu_freq'] = psutil.cpu_freq()
    
    # Memory information
    memory = psutil.virtual_memory()
    info['memory_total'] = memory.total / (1024**3)  # GB
    info['memory_available'] = memory.available / (1024**3)  # GB
    info['memory_percent'] = memory.percent
    
    # Disk information
    disk = psutil.disk_usage('/')
    info['disk_total'] = disk.total / (1024**3)  # GB
    info['disk_free'] = disk.free / (1024**3)  # GB
    
    return info

def check_python_packages():
    """Check for AI/ML packages"""
    print("\nğŸ“¦ ÙØ­Øµ Ø§Ù„Ù…ÙƒØªØ¨Ø§Øª Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø©")
    print("Checking Required Packages")
    print("=" * 35)
    
    packages_to_check = [
        'torch',
        'transformers', 
        'numpy',
        'pandas',
        'requests',
        'arabic_reshaper',
        'bidi'
    ]
    
    installed = {}
    for package in packages_to_check:
        try:
            __import__(package)
            installed[package] = "âœ… Ù…Ø«Ø¨Øª"
            print(f"{package}: âœ… Ù…Ø«Ø¨Øª")
        except ImportError:
            installed[package] = "âŒ ØºÙŠØ± Ù…Ø«Ø¨Øª"
            print(f"{package}: âŒ ØºÙŠØ± Ù…Ø«Ø¨Øª")
    
    return installed

def test_cpu_performance():
    """Test CPU performance for AI workloads"""
    print("\nâš¡ Ø§Ø®ØªØ¨Ø§Ø± Ø£Ø¯Ø§Ø¡ Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬")
    print("CPU Performance Test")
    print("=" * 25)
    
    try:
        import numpy as np
        import time
        
        # Matrix multiplication test
        size = 500
        print(f"ğŸ§® Ø¶Ø±Ø¨ Ù…ØµÙÙˆÙØ§Øª {size}x{size}...")
        
        start_time = time.time()
        a = np.random.randn(size, size)
        b = np.random.randn(size, size)
        c = np.dot(a, b)
        end_time = time.time()
        
        cpu_time = end_time - start_time
        print(f"â±ï¸ Ø§Ù„ÙˆÙ‚Øª: {cpu_time:.3f} Ø«Ø§Ù†ÙŠØ©")
        print(f"Time: {cpu_time:.3f} seconds")
        
        # Estimate model performance
        if cpu_time < 0.1:
            performance = "Ù…Ù…ØªØ§Ø² - Ù…Ù†Ø§Ø³Ø¨ Ù„Ù„Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„ÙƒØ¨ÙŠØ±Ø©"
            performance_en = "Excellent - suitable for large models"
        elif cpu_time < 0.5:
            performance = "Ø¬ÙŠØ¯ - Ù…Ù†Ø§Ø³Ø¨ Ù„Ù„Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„Ù…ØªÙˆØ³Ø·Ø©"
            performance_en = "Good - suitable for medium models"
        elif cpu_time < 2.0:
            performance = "Ù…Ù‚Ø¨ÙˆÙ„ - Ù…Ù†Ø§Ø³Ø¨ Ù„Ù„Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„ØµØºÙŠØ±Ø©"
            performance_en = "Acceptable - suitable for small models"
        else:
            performance = "Ø¨Ø·ÙŠØ¡ - Ù‚Ø¯ ØªØ­ØªØ§Ø¬ ØªØ­Ø³ÙŠÙ†"
            performance_en = "Slow - may need optimization"
        
        print(f"ğŸ“Š Ø§Ù„ØªÙ‚ÙŠÙŠÙ…: {performance}")
        print(f"Assessment: {performance_en}")
        
        return {
            'matrix_time': cpu_time,
            'performance': performance,
            'performance_en': performance_en
        }
        
    except ImportError:
        print("âŒ NumPy ØºÙŠØ± Ù…ØªØ§Ø­ Ù„Ù„Ø§Ø®ØªØ¨Ø§Ø±")
        return None

def recommend_model_sizes(system_info):
    """Recommend appropriate model sizes based on system specs"""
    print("\nğŸ¯ ØªÙˆØµÙŠØ§Øª Ø§Ù„Ù†Ù…Ø§Ø°Ø¬")
    print("Model Recommendations")
    print("=" * 25)
    
    memory_gb = system_info['memory_available']
    cpu_cores = system_info['cpu_count_logical']
    
    print(f"ğŸ’¾ Ø§Ù„Ø°Ø§ÙƒØ±Ø© Ø§Ù„Ù…ØªØ§Ø­Ø©: {memory_gb:.1f} GB")
    print(f"Available Memory: {memory_gb:.1f} GB")
    print(f"ğŸ”§ Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø§Øª: {cpu_cores} cores")
    print(f"CPU Cores: {cpu_cores}")
    
    recommendations = []
    
    if memory_gb >= 6:
        recommendations.append({
            'model': 'llama3.2:3b',
            'size': '2.0 GB',
            'reason': 'Ù…ØªÙˆØ§Ø²Ù† - Ø¬ÙˆØ¯Ø© Ø¬ÙŠØ¯Ø© ÙˆØ³Ø±Ø¹Ø© Ù…Ø¹Ù‚ÙˆÙ„Ø©',
            'reason_en': 'Balanced - good quality and reasonable speed'
        })
    
    if memory_gb >= 3:
        recommendations.append({
            'model': 'llama3.2:1b', 
            'size': '1.3 GB',
            'reason': 'Ø³Ø±ÙŠØ¹ - Ù…Ù†Ø§Ø³Ø¨ Ù„Ù„Ø§Ø®ØªØ¨Ø§Ø± ÙˆØ§Ù„ØªØ·ÙˆÙŠØ±',
            'reason_en': 'Fast - suitable for testing and development'
        })
    
    if memory_gb >= 8:
        recommendations.append({
            'model': 'llama3.1:8b',
            'size': '4.7 GB', 
            'reason': 'Ø¬ÙˆØ¯Ø© Ø¹Ø§Ù„ÙŠØ© - Ù„Ù„Ù…Ù‡Ø§Ù… Ø§Ù„Ù…ØªÙ‚Ø¯Ù…Ø©',
            'reason_en': 'High quality - for advanced tasks'
        })
    
    if not recommendations:
        recommendations.append({
            'model': 'tinyllama:1.1b',
            'size': '0.6 GB',
            'reason': 'Ø®ÙÙŠÙ Ø¬Ø¯Ø§Ù‹ - Ù„Ù„Ø£Ù†Ø¸Ù…Ø© Ø§Ù„Ù…Ø­Ø¯ÙˆØ¯Ø©',
            'reason_en': 'Very light - for limited systems'
        })
    
    print("\nğŸ“‹ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„Ù…ÙÙˆØµÙ‰ Ø¨Ù‡Ø§:")
    print("Recommended Models:")
    for rec in recommendations:
        print(f"  ğŸ¦™ {rec['model']} ({rec['size']})")
        print(f"     {rec['reason']}")
        print(f"     {rec['reason_en']}")
        print()
    
    return recommendations

def check_ollama_optimization():
    """Check Ollama optimization settings"""
    print("\nğŸ”§ ØªØ­Ø³ÙŠÙ† Ollama")
    print("Ollama Optimization")
    print("=" * 20)
    
    # Check if Ollama is running
    try:
        result = subprocess.run(['ollama', 'ps'], capture_output=True, text=True, timeout=5)
        if result.returncode == 0:
            print("âœ… Ollama ÙŠØ¹Ù…Ù„")
            print("Ollama is running")
        else:
            print("âš ï¸ Ollama ØºÙŠØ± Ù…ÙØ´ØºÙ„")
            print("Ollama not running")
            print("ğŸ’¡ Ù„ØªØ´ØºÙŠÙ„Ù‡: ollama serve")
            print("To start: ollama serve")
    except (subprocess.TimeoutExpired, FileNotFoundError):
        print("âŒ Ollama ØºÙŠØ± Ù…ØªØ§Ø­")
        print("Ollama not available")
        return False
    
    # Environment variables for optimization
    optimizations = {
        'OLLAMA_NUM_PARALLEL': 'Ø¹Ø¯Ø¯ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„Ù…ØªÙˆØ§Ø²ÙŠØ©',
        'OLLAMA_MAX_LOADED_MODELS': 'Ø£Ù‚ØµÙ‰ Ø¹Ø¯Ø¯ Ù†Ù…Ø§Ø°Ø¬ Ù…Ø­Ù…Ù„Ø©',
        'OLLAMA_FLASH_ATTENTION': 'ØªÙØ¹ÙŠÙ„ Flash Attention',
        'OLLAMA_LLM_LIBRARY': 'Ù…ÙƒØªØ¨Ø© LLM Ø§Ù„Ù…ÙØ¶Ù„Ø©'
    }
    
    print("\nğŸ› ï¸ Ù…ØªØºÙŠØ±Ø§Øª Ø§Ù„ØªØ­Ø³ÙŠÙ†:")
    print("Optimization Variables:")
    for var, desc in optimizations.items():
        value = os.environ.get(var, 'ØºÙŠØ± Ù…Ø­Ø¯Ø¯ / not set')
        print(f"  {var}: {value}")
        print(f"    ({desc})")
    
    return True

def generate_system_report():
    """Generate comprehensive system report"""
    print("\nğŸ“‹ Ø¥Ù†Ø´Ø§Ø¡ ØªÙ‚Ø±ÙŠØ± Ø´Ø§Ù…Ù„...")
    print("Generating comprehensive report...")
    
    system_info = get_system_info()
    packages = check_python_packages()
    cpu_performance = test_cpu_performance()
    recommendations = recommend_model_sizes(system_info)
    
    report = {
        'system_info': system_info,
        'packages': packages,
        'cpu_performance': cpu_performance,
        'recommendations': recommendations,
        'timestamp': str(psutil.boot_time())
    }
    
    # Save report
    report_file = Path('system_capabilities_report.txt')
    with open(report_file, 'w', encoding='utf-8') as f:
        f.write("ØªÙ‚Ø±ÙŠØ± Ù‚Ø¯Ø±Ø§Øª Ø§Ù„Ù†Ø¸Ø§Ù… Ù„Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ\n")
        f.write("System Capabilities Report for AI\n")
        f.write("=" * 50 + "\n\n")
        
        f.write("Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ù†Ø¸Ø§Ù… / System Info:\n")
        for key, value in system_info.items():
            f.write(f"  {key}: {value}\n")
        
        f.write(f"\nØ§Ù„Ø°Ø§ÙƒØ±Ø© Ø§Ù„Ù…ØªØ§Ø­Ø© / Available Memory: {system_info['memory_available']:.1f} GB\n")
        f.write(f"Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø§Øª / CPU Cores: {system_info['cpu_count_logical']}\n")
        
        f.write("\nØ§Ù„Ù…ÙƒØªØ¨Ø§Øª Ø§Ù„Ù…Ø«Ø¨ØªØ© / Installed Packages:\n")
        for pkg, status in packages.items():
            f.write(f"  {pkg}: {status}\n")
        
        if cpu_performance:
            f.write(f"\nØ£Ø¯Ø§Ø¡ Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬ / CPU Performance:\n")
            f.write(f"  Matrix multiplication time: {cpu_performance['matrix_time']:.3f}s\n")
            f.write(f"  Assessment: {cpu_performance['performance']}\n")
        
        f.write(f"\nØ§Ù„Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„Ù…ÙÙˆØµÙ‰ Ø¨Ù‡Ø§ / Recommended Models:\n")
        for rec in recommendations:
            f.write(f"  {rec['model']} ({rec['size']}) - {rec['reason']}\n")
    
    print(f"ğŸ’¾ ØªÙ… Ø­ÙØ¸ Ø§Ù„ØªÙ‚Ø±ÙŠØ± ÙÙŠ: {report_file}")
    print(f"Report saved to: {report_file}")
    
    return report

def main():
    print("ğŸ–¥ï¸ ÙØ­Øµ Ù‚Ø¯Ø±Ø§Øª Ø§Ù„Ù†Ø¸Ø§Ù… Ù„Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ")
    print("AI System Capabilities Check")
    print("=" * 50)
    
    # Get system information
    system_info = get_system_info()
    
    print(f"ğŸ—ï¸ Ø§Ù„Ù…Ø¹Ù…Ø§Ø±ÙŠØ©: {system_info['architecture']}")
    print(f"Architecture: {system_info['architecture']}")
    print(f"ğŸ§  Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬: {system_info['processor']}")
    print(f"Processor: {system_info['processor']}")
    print(f"ğŸ’¾ Ø§Ù„Ø°Ø§ÙƒØ±Ø©: {system_info['memory_total']:.1f} GB (Ù…ØªØ§Ø­: {system_info['memory_available']:.1f} GB)")
    print(f"Memory: {system_info['memory_total']:.1f} GB (available: {system_info['memory_available']:.1f} GB)")
    print(f"ğŸ”§ Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø§Øª: {system_info['cpu_count']} ÙÙŠØ²ÙŠØ§Ø¦ÙŠØŒ {system_info['cpu_count_logical']} Ù…Ù†Ø·Ù‚ÙŠ")
    print(f"CPUs: {system_info['cpu_count']} physical, {system_info['cpu_count_logical']} logical")
    
    # Check packages
    packages = check_python_packages()
    
    # Test performance
    cpu_performance = test_cpu_performance()
    
    # Get recommendations
    recommendations = recommend_model_sizes(system_info)
    
    # Check Ollama
    check_ollama_optimization()
    
    # Generate report
    generate_system_report()
    
    print("\nğŸ‰ Ø§Ù„Ø®Ù„Ø§ØµØ©:")
    print("Summary:")
    print("âœ… Ø§Ù„Ù†Ø¸Ø§Ù… Ù‚Ø§Ø¯Ø± Ø¹Ù„Ù‰ ØªØ´ØºÙŠÙ„ Ù†Ù…Ø§Ø°Ø¬ Llama")
    print("System can run Llama models")
    print("ğŸ’¡ Ø§Ø³ØªØ®Ø¯Ù… Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„ØµØºÙŠØ±Ø© Ù„Ù„Ø¨Ø¯Ø§ÙŠØ©")
    print("Use smaller models to start")
    print("ğŸš€ ÙŠÙ…ÙƒÙ† Ø§Ù„ØªØ¯Ø±ÙŠØ¨ Ø¹Ù„Ù‰ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø®Ø§ØµØ©")
    print("Can train on custom data")

if __name__ == "__main__":
    main()
