"""
Fine-tuning Llama Models with Custom Data
Ø¶Ø¨Ø· Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„Ù„Ø§Ù…Ø§ Ø¨Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ø®ØµØµØ©

This script provides tools for fine-tuning Llama models with your custom text data.
"""

import os
import json
import torch
from pathlib import Path
from typing import Optional, Dict, List
import subprocess

class LlamaFineTuner:
    def __init__(self, model_name: str = "llama3.2:3b"):
        self.model_name = model_name
        self.training_dir = Path("training_data")
        self.training_dir.mkdir(exist_ok=True)
        
    def create_modelfile(self, system_prompt: str, training_data_path: str) -> str:
        """
        Create a Modelfile for Ollama fine-tuning
        Ø¥Ù†Ø´Ø§Ø¡ Ù…Ù„Ù Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ù„Ù„Ø¶Ø¨Ø· Ø§Ù„Ø¯Ù‚ÙŠÙ‚ ÙÙŠ Ollama
        """
        modelfile_content = f"""FROM {self.model_name}

# System prompt in Arabic and English
SYSTEM \"\"\"
{system_prompt}

Ø£Ù†Øª Ù…Ø³Ø§Ø¹Ø¯ Ø°ÙƒÙŠ Ù…ÙØ¯Ø±Ø¨ Ø¹Ù„Ù‰ Ø¨ÙŠØ§Ù†Ø§Øª Ù…Ø®ØµØµØ©. ØªØ¬ÙŠØ¨ Ø¨Ø¯Ù‚Ø© ÙˆØ¨Ø£Ø³Ù„ÙˆØ¨ Ù…ÙÙŠØ¯.
You are an intelligent assistant trained on custom data. You respond accurately and helpfully.
\"\"\"

# Training parameters
PARAMETER temperature 0.7
PARAMETER top_p 0.9
PARAMETER top_k 40
PARAMETER repeat_penalty 1.1

# Custom training data
ADAPTER {training_data_path}
"""
        
        modelfile_path = self.training_dir / "Modelfile"
        with open(modelfile_path, 'w', encoding='utf-8') as f:
            f.write(modelfile_content)
        
        return str(modelfile_path)
    
    def fine_tune_with_ollama(self, training_file: str, custom_model_name: str, 
                             system_prompt: str = None) -> bool:
        """
        Fine-tune using Ollama's built-in capabilities
        Ø§Ù„Ø¶Ø¨Ø· Ø§Ù„Ø¯Ù‚ÙŠÙ‚ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø¥Ù…ÙƒØ§Ù†ÙŠØ§Øª Ollama Ø§Ù„Ù…Ø¯Ù…Ø¬Ø©
        """
        if not system_prompt:
            system_prompt = "Ø£Ù†Øª Ù…Ø³Ø§Ø¹Ø¯ Ø°ÙƒÙŠ Ù…ÙØ¯Ø±Ø¨ Ø¹Ù„Ù‰ Ø¨ÙŠØ§Ù†Ø§Øª Ù…Ø®ØµØµØ©"
        
        print(f"ğŸ”§ Fine-tuning {self.model_name} with {training_file}")
        print(f"ğŸ“ Creating custom model: {custom_model_name}")
        
        try:
            # Create Modelfile
            modelfile_path = self.create_modelfile(system_prompt, training_file)
            print(f"âœ… Created Modelfile: {modelfile_path}")
            
            # Create custom model with Ollama
            cmd = ["ollama", "create", custom_model_name, "-f", modelfile_path]
            result = subprocess.run(cmd, capture_output=True, text=True, check=True)
            
            print(f"âœ… Successfully created custom model: {custom_model_name}")
            print("ğŸ¯ You can now use it with:")
            print(f"   ollama run {custom_model_name}")
            
            return True
            
        except subprocess.CalledProcessError as e:
            print(f"âŒ Error creating model: {e}")
            print(f"Output: {e.stdout}")
            print(f"Error: {e.stderr}")
            return False
    
    def prepare_training_examples(self, jsonl_file: str) -> List[str]:
        """
        Convert JSONL training data to examples for the model
        ØªØ­ÙˆÙŠÙ„ Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ØªØ¯Ø±ÙŠØ¨ Ø¥Ù„Ù‰ Ø£Ù…Ø«Ù„Ø© Ù„Ù„Ù†Ù…ÙˆØ°Ø¬
        """
        examples = []
        
        with open(jsonl_file, 'r', encoding='utf-8') as f:
            for line in f:
                data = json.loads(line)
                
                if 'messages' in data:  # Conversation format
                    conversation = ""
                    for msg in data['messages']:
                        if msg['role'] == 'user':
                            conversation += f"Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…: {msg['content']}\n"
                        elif msg['role'] == 'assistant':
                            conversation += f"Ø§Ù„Ù…Ø³Ø§Ø¹Ø¯: {msg['content']}\n"
                    examples.append(conversation)
                
                elif 'instruction' in data:  # Instruction format
                    example = f"Ø§Ù„ØªØ¹Ù„ÙŠÙ…Ø§Øª: {data['instruction']}\n"
                    if 'input' in data and data['input']:
                        example += f"Ø§Ù„Ù…Ø¯Ø®Ù„: {data['input']}\n"
                    example += f"Ø§Ù„Ø¥Ø®Ø±Ø§Ø¬: {data['output']}\n"
                    examples.append(example)
                
                elif 'prompt' in data:  # Completion format
                    example = f"Ø§Ù„Ù…Ø·Ù„ÙˆØ¨: {data['prompt']}\nØ§Ù„Ø¥Ø¬Ø§Ø¨Ø©: {data['completion']}\n"
                    examples.append(example)
        
        return examples
    
    def test_custom_model(self, model_name: str, test_prompts: List[str]) -> Dict:
        """
        Test the custom model with sample prompts
        Ø§Ø®ØªØ¨Ø§Ø± Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ø®ØµØµ Ø¨Ø¹ÙŠÙ†Ø§Øª
        """
        from llama_api import LlamaAPI
        
        api = LlamaAPI()
        results = {}
        
        print(f"ğŸ§ª Testing custom model: {model_name}")
        print("=" * 40)
        
        for i, prompt in enumerate(test_prompts, 1):
            print(f"\nTest {i}: {prompt}")
            print("-" * 30)
            
            try:
                response = api.generate(
                    model=model_name,
                    prompt=prompt,
                    options={"temperature": 0.7}
                )
                
                answer = response['response']
                print(f"Response: {answer}")
                results[f"test_{i}"] = {
                    "prompt": prompt,
                    "response": answer,
                    "success": True
                }
                
            except Exception as e:
                print(f"Error: {e}")
                results[f"test_{i}"] = {
                    "prompt": prompt,
                    "error": str(e),
                    "success": False
                }
        
        return results

def create_sample_training_data():
    """Create sample training data in Arabic"""
    sample_conversations = [
        {
            "messages": [
                {"role": "system", "content": "Ø£Ù†Øª Ù…Ø³Ø§Ø¹Ø¯ Ø°ÙƒÙŠ Ù…ØªØ®ØµØµ ÙÙŠ Ø§Ù„ØªÙƒÙ†ÙˆÙ„ÙˆØ¬ÙŠØ§"},
                {"role": "user", "content": "Ù…Ø§ Ù‡Ùˆ Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠØŸ"},
                {"role": "assistant", "content": "Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ Ù‡Ùˆ Ù…Ø¬Ø§Ù„ ÙÙŠ Ø¹Ù„ÙˆÙ… Ø§Ù„Ø­Ø§Ø³ÙˆØ¨ ÙŠÙ‡Ø¯Ù Ø¥Ù„Ù‰ Ø¥Ù†Ø´Ø§Ø¡ Ø£Ù†Ø¸Ù…Ø© Ù‚Ø§Ø¯Ø±Ø© Ø¹Ù„Ù‰ Ø£Ø¯Ø§Ø¡ Ù…Ù‡Ø§Ù… ØªØªØ·Ù„Ø¨ Ø¹Ø§Ø¯Ø© Ø°ÙƒØ§Ø¡Ù‹ Ø¨Ø´Ø±ÙŠØ§Ù‹ØŒ Ù…Ø«Ù„ Ø§Ù„ØªØ¹Ù„Ù… ÙˆØ§ØªØ®Ø§Ø° Ø§Ù„Ù‚Ø±Ø§Ø±Ø§Øª ÙˆØ­Ù„ Ø§Ù„Ù…Ø´ÙƒÙ„Ø§Øª."}
            ]
        },
        {
            "messages": [
                {"role": "system", "content": "Ø£Ù†Øª Ù…Ø³Ø§Ø¹Ø¯ Ø°ÙƒÙŠ Ù…ØªØ®ØµØµ ÙÙŠ Ø§Ù„ØªÙƒÙ†ÙˆÙ„ÙˆØ¬ÙŠØ§"},
                {"role": "user", "content": "ÙƒÙŠÙ ÙŠØ¹Ù…Ù„ Ø§Ù„ØªØ¹Ù„Ù… Ø§Ù„Ø¢Ù„ÙŠØŸ"},
                {"role": "assistant", "content": "Ø§Ù„ØªØ¹Ù„Ù… Ø§Ù„Ø¢Ù„ÙŠ ÙŠØ¹Ù…Ù„ Ù…Ù† Ø®Ù„Ø§Ù„ ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ Ø¹Ù„Ù‰ ÙƒÙ…ÙŠØ§Øª ÙƒØ¨ÙŠØ±Ø© Ù…Ù† Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù„ØªØªØ¹Ù„Ù… Ø§Ù„Ø£Ù†Ù…Ø§Ø· ÙˆØ§Ù„Ø¹Ù„Ø§Ù‚Ø§ØªØŒ Ø«Ù… ØªØ³ØªØ®Ø¯Ù… Ù‡Ø°Ù‡ Ø§Ù„Ù…Ø¹Ø±ÙØ© Ù„Ù„ØªÙ†Ø¨Ø¤ Ø£Ùˆ Ø§ØªØ®Ø§Ø° Ù‚Ø±Ø§Ø±Ø§Øª Ø¨Ø´Ø£Ù† Ø¨ÙŠØ§Ù†Ø§Øª Ø¬Ø¯ÙŠØ¯Ø©."}
            ]
        }
    ]
    
    # Save sample data
    training_dir = Path("training_data")
    training_dir.mkdir(exist_ok=True)
    
    sample_file = training_dir / "sample_conversations.jsonl"
    with open(sample_file, 'w', encoding='utf-8') as f:
        for conv in sample_conversations:
            f.write(json.dumps(conv, ensure_ascii=False) + '\n')
    
    print(f"âœ… Created sample training data: {sample_file}")
    return str(sample_file)

def main():
    print("ğŸ¦™ Ø¶Ø¨Ø· Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„Ù„Ø§Ù…Ø§ Ø¨Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ø®ØµØµØ©")
    print("Llama Fine-tuning with Custom Data")
    print("=" * 50)
    
    fine_tuner = LlamaFineTuner()
    
    print("\nØ§Ø®ØªØ± Ù…Ø§ ØªØ±ÙŠØ¯ ÙØ¹Ù„Ù‡:")
    print("1. Ø¥Ù†Ø´Ø§Ø¡ Ø¨ÙŠØ§Ù†Ø§Øª ØªØ¯Ø±ÙŠØ¨ Ø¹ÙŠÙ†Ø© (Create sample training data)")
    print("2. Ø¶Ø¨Ø· Ù†Ù…ÙˆØ°Ø¬ Ø¨Ù…Ù„Ù Ù…ÙˆØ¬ÙˆØ¯ (Fine-tune with existing file)")
    print("3. Ø§Ø®ØªØ¨Ø§Ø± Ù†Ù…ÙˆØ°Ø¬ Ù…Ø®ØµØµ (Test custom model)")
    print("4. Ø®Ø±ÙˆØ¬ (Exit)")
    
    choice = input("\nØ§Ø®ØªØ± Ø±Ù‚Ù…Ø§Ù‹ (1-4): ").strip()
    
    if choice == "1":
        sample_file = create_sample_training_data()
        print(f"\nğŸ“ Ù…Ù„Ù Ø§Ù„Ø¹ÙŠÙ†Ø©: {sample_file}")
        print("ÙŠÙ…ÙƒÙ†Ùƒ Ø§Ù„Ø¢Ù† Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡ Ù„Ù„Ø¶Ø¨Ø· Ø§Ù„Ø¯Ù‚ÙŠÙ‚")
    
    elif choice == "2":
        # List available training files
        training_files = list(Path("training_data").glob("*.jsonl"))
        
        if not training_files:
            print("âŒ Ù„Ø§ ØªÙˆØ¬Ø¯ Ù…Ù„ÙØ§Øª ØªØ¯Ø±ÙŠØ¨. Ø§Ø³ØªØ®Ø¯Ù… text_data_processor.py Ø£ÙˆÙ„Ø§Ù‹")
            return
        
        print("\nÙ…Ù„ÙØ§Øª Ø§Ù„ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ù…ØªØ§Ø­Ø©:")
        for i, file in enumerate(training_files, 1):
            print(f"{i}. {file.name}")
        
        try:
            file_choice = int(input("Ø§Ø®ØªØ± Ø±Ù‚Ù… Ø§Ù„Ù…Ù„Ù: ")) - 1
            selected_file = training_files[file_choice]
            
            custom_name = input("Ø§Ø³Ù… Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ø®ØµØµ: ").strip()
            if not custom_name:
                custom_name = f"custom-llama-{selected_file.stem}"
            
            system_prompt = input("Ø§Ù„Ù†Ø¸Ø§Ù… Ø§Ù„Ø£Ø³Ø§Ø³ÙŠ (Ø§Ø®ØªÙŠØ§Ø±ÙŠ): ").strip()
            
            success = fine_tuner.fine_tune_with_ollama(
                str(selected_file), 
                custom_name, 
                system_prompt if system_prompt else None
            )
            
            if success:
                print(f"\nğŸ‰ ØªÙ… Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ø®ØµØµ: {custom_name}")
                
        except (ValueError, IndexError):
            print("âŒ Ø§Ø®ØªÙŠØ§Ø± ØºÙŠØ± ØµØ­ÙŠØ­")
    
    elif choice == "3":
        model_name = input("Ø§Ø³Ù… Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ø®ØµØµ Ù„Ù„Ø§Ø®ØªØ¨Ø§Ø±: ").strip()
        
        test_prompts = [
            "Ø§Ø´Ø±Ø­ Ù„ÙŠ Ù…ÙÙ‡ÙˆÙ… Ø§Ù„Ø¨Ø±Ù…Ø¬Ø©",
            "Ù…Ø§ Ù‡ÙŠ Ø£ÙØ¶Ù„ Ù„ØºØ§Øª Ø§Ù„Ø¨Ø±Ù…Ø¬Ø©ØŸ",
            "ÙƒÙŠÙ Ø£ØªØ¹Ù„Ù… Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠØŸ"
        ]
        
        results = fine_tuner.test_custom_model(model_name, test_prompts)
        
        print(f"\nğŸ“Š Ù†ØªØ§Ø¦Ø¬ Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø±:")
        success_count = sum(1 for r in results.values() if r.get('success'))
        print(f"Ù†Ø¬Ø­ {success_count} Ù…Ù† {len(results)} Ø§Ø®ØªØ¨Ø§Ø±Ø§Øª")
    
    elif choice == "4":
        print("ÙˆØ¯Ø§Ø¹Ø§Ù‹! ğŸ‘‹")
    
    else:
        print("âŒ Ø§Ø®ØªÙŠØ§Ø± ØºÙŠØ± ØµØ­ÙŠØ­")

if __name__ == "__main__":
    main()
