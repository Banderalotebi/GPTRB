# ü¶ô Llama 3.1/3.2 Comprehensive Setup

This repository provides a complete setup for working with Meta's Llama 3.1 and 3.2 models locally. These are among the best open-source language models available, offering excellent performance across various tasks.

## üåü What's Included

- **Complete setup automation** for Llama models
- **Python API interface** for easy integration
- **Multiple usage examples** covering different use cases
- **Interactive chat interface**
- **Code generation, text analysis, and more**

## üöÄ Quick Start

### 1. Automated Setup
```bash
./setup_llama.sh
```

### 2. Manual Setup
```bash
# Install dependencies
pip install -r requirements.txt

# Install Ollama (if not already installed)
curl -fsSL https://ollama.com/install.sh | sh

# Start Ollama service
ollama serve &

# Download a model (choose one)
ollama pull llama3.2:1b    # Fastest (1.3GB)
ollama pull llama3.2:3b    # Balanced (2.0GB)
ollama pull llama3.1:8b    # High quality (4.7GB)
```

## üìÅ Files Overview

| File | Description |
|------|-------------|
| `llama_setup.py` | Interactive setup and model management tool |
| `llama_api.py` | Python API for working with Llama models |
| `llama_examples.py` | Comprehensive examples and use cases |
| `setup_llama.sh` | Automated setup script |

## üéØ Usage Examples

### Basic Chat
```python
from llama_api import LlamaAPI

api = LlamaAPI()
response = api.generate(
    model="llama3.2:3b",
    prompt="Explain quantum computing simply"
)
print(response['response'])
```

### Conversation Style
```python
messages = [
    {"role": "system", "content": "You are a helpful assistant."},
    {"role": "user", "content": "What are Llama models?"}
]

response = api.chat(model="llama3.2:3b", messages=messages)
print(response['message']['content'])
```

### Streaming Responses
```python
for chunk in api.generate(model="llama3.2:3b", prompt="Write a story", stream=True):
    if 'response' in chunk:
        print(chunk['response'], end='', flush=True)
```

## üß™ Try the Examples

Run the interactive examples:
```bash
python3 llama_examples.py
```

Features include:
- **Code Generation** - Generate code in any programming language
- **Text Analysis** - Sentiment analysis, topic extraction, summarization
- **Creative Writing** - Stories, poems, creative content
- **Data Extraction** - Extract structured data from unstructured text
- **Question Answering** - Answer questions based on context
- **Language Translation** - Translate between languages
- **Interactive Chat** - Full conversation interface

## üîß Model Management

Use the setup tool for easy model management:
```bash
python3 llama_setup.py
```

This provides:
- Model download and installation
- List installed models
- Interactive chat interface
- Model information and stats

## üìä Model Comparison

| Model | Size | Speed | Quality | Use Case |
|-------|------|-------|---------|----------|
| Llama 3.2 1B | 1.3GB | ‚ö°‚ö°‚ö° | ‚≠ê‚≠ê | Testing, quick tasks |
| Llama 3.2 3B | 2.0GB | ‚ö°‚ö° | ‚≠ê‚≠ê‚≠ê | Balanced performance |
| Llama 3.1 8B | 4.7GB | ‚ö° | ‚≠ê‚≠ê‚≠ê‚≠ê | High-quality tasks |
| Llama 3.1 70B | 40GB | üêå | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | Best quality |

## üõ†Ô∏è Advanced Usage

### Custom Parameters
```python
response = api.generate(
    model="llama3.2:3b",
    prompt="Your prompt here",
    options={
        "temperature": 0.7,    # Creativity (0.0-1.0)
        "top_p": 0.9,         # Nucleus sampling
        "top_k": 40,          # Top-k sampling
        "repeat_penalty": 1.1  # Repetition penalty
    }
)
```

### Fine-tuning (Advanced)
For custom fine-tuning, check the Hugging Face Transformers examples in the codebase.

## üåê API Endpoints

When Ollama is running, it provides a REST API at `http://localhost:11434`:

- `POST /api/generate` - Generate text
- `POST /api/chat` - Chat conversations
- `GET /api/tags` - List models
- `POST /api/pull` - Download models
- `POST /api/show` - Model information

## üîç Troubleshooting

### Common Issues

1. **Ollama not found**: Make sure Ollama is installed and in your PATH
2. **Connection refused**: Start Ollama service with `ollama serve`
3. **Model not found**: Download the model first with `ollama pull model_name`
4. **Out of memory**: Try a smaller model (e.g., llama3.2:1b)

### Performance Tips

- Use smaller models for development and testing
- Enable GPU acceleration if available
- Adjust temperature for different use cases (lower for factual, higher for creative)
- Use streaming for long responses

## üìù License

Llama models are released under Meta's custom license. Check the official Llama repository for details.

## ü§ù Contributing

Feel free to submit issues and enhancement requests!

## üìö Resources

- [Ollama Documentation](https://ollama.ai/docs)
- [Llama Official Repository](https://github.com/meta-llama/llama)
- [Hugging Face Transformers](https://huggingface.co/docs/transformers)